extends: base
secret_key: "llama-2-70b-hf"

serving_runtime:
  transformer:
    resource_request:
      cpu: 4
      memory: 8 # in Gi
    extra_env:
      RUNTIME_GRPC_SERVER_THREAD_POOL_SIZE: 160
  kserve:
    resource_request:
      cpu: 8
      memory: 150 # in Gi
      nvidia.com/gpu: 8
      nvidia.com/gpu_memory: 320 # in Gi of GPU memory
    extra_env:
      FLASH_ATTENTION: true
      DEPLOYMENT_FRAMEWORK: hf_custom_tp
      RAYON_NUM_THREADS: "64" # Would otherwise be calculated based on shards*num_cpus
inference_service:
  storage_uri: "s3://psap-watsonx-models/Llama-2-70b-hf/Llama-2-70b-hf"

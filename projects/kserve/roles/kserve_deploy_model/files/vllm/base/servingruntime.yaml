apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-servingruntime # Overwritten by templates
  labels:
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/template-display-name: "ServingRuntime for vLLM | Topsail"
spec:
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
  - args: # Merged with args in inferenceservice
    - --model=/mnt/models/
    - --download-dir=/models-cache
    - --port=8080
    image: quay.io/opendatahub/vllm:fast-ibm-nightly-2024-05-01 # Overwritten by templates
    name: kserve-container
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP
    resources:
      requests: # Overwritten by InferenceService
        cpu: 4
        memory: 8Gi
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: pytorch

apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ wisdom_deploy_model_serving_runtime_name }}
  namespace: {{ wisdom_deploy_model_namespace }}
spec:
  replicas: {{ wisdom_deploy_model_replicas }}
  grpcDataEndpoint: port:8087
  grpcEndpoint: port:8087
  multiModel: true
  storageHelper:
    disabled: false
  supportedModelFormats:
    - autoSelect: true
      name: wisdom
  containers:
    - name: runtime
      #TODO: Make this a variable
      image: quay.io/rhods-wisdom/fmaas-runtime-wisdom-ansible:0.30.0_ubi8_py39
      env:
        - name: ACCEPT_LICENSE
          value: "true"
        - name: LOG_LEVEL
          value: debug3
        # CAPACITY and DEFAULT_MODEL_SIZE numbers for pre-converted ONNX models with reported size ~41081MiB
        - name: INFERENCE_PLUGIN_MODEL_MESH_CAPACITY
          value: "62914560000"
        - name: INFERENCE_PLUGIN_MODEL_MESH_DEFAULT_MODEL_SIZE
          value: "31457280000"

        # limits model loads/unloads from ModelMesh
        - name: INFERENCE_PLUGIN_MODEL_MESH_MAX_LOADING_CONCURRENCY
          value: "1"

        - name: RUNTIME_PORT
          value: "8087"
        - name: GATEWAY_PORT
          value: "8060"
        - name: RUNTIME_METRICS_PORT
          value: "2113"
        - name: RUNTIME_LOCAL_MODELS_DIR
          value: "/models/"

        # use RPM based scaling in ModelMesh
        # FIXME: these configs will be the default in the release after 0.21.0
        - name: INFERENCE_PLUGIN_MODEL_MESH_LATENCY_BASED_AUTOSCALING_ENABLED
          value: "false"
        - name: RUNTIME_SERVER_THREAD_POOL_SIZE
          value: "16"

        # TGIS env vars
        - name: CUDA_PAD_TO_MULT_OF_8
          value: "false"
        - name: MAX_BATCH_SIZE
          value: "16"
        - name: MAX_SEQUENCE_LENGTH
          value: "2048"
        - name: NUM_GPUS
          value: "1"
        - name: MERGE_ONNX_GRAPHS
          value: "true"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: TRANSFORMERS_CACHE
          value: /tmp/transformers_cache
        - name: HUGGINGFACE_HUB_CACHE
          value: /tmp/transformers_cache
        - name: MAX_CONCURRENT_REQUESTS
          value: "64"
        - name: DEPLOYMENT_FRAMEWORK
          value: hf_optimum_ort
      ports:
        - containerPort: 8087
          name: runtime-grpc
          protocol: TCP
        - containerPort: 8060
          name: runtime-rest
          protocol: TCP
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          cpu: 4
          memory: "8Gi"
          nvidia.com/gpu: "1"
